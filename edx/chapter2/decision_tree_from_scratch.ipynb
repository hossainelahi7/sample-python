{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Decision Tree from scratch\n",
                "\n",
                "In this exercise we'll build a portion of a decision tree classifier from scratch and compare our results to the `sklearn` implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import sklearn as sk\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn import tree\n",
                "from sklearn.tree import DecisionTreeClassifier"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Our toy dataset consists of two predictors and a binary class label."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read the dataframe in\n",
                "tree_df = pd.read_csv('two_classes.csv')\n",
                "# Inspect the top 5 rows\n",
                "tree_df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Visualizing the data we can imagine what the ideal decision boundaries might look like."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the data to visualize class patterns\n",
                "\n",
                "# Create figure of specific size and the axes objects\n",
                "fig, ax = plt.subplots(figsize=(6,6))\n",
                "\n",
                "# Scatter the data points\n",
                "# Other colormaps could be found here:\n",
                "# https://stackoverflow.com/questions/34314356/how-to-view-all-colormaps-available-in-matplotlib\n",
                "scatter = ax.scatter(tree_df['x1'], tree_df['x2'], c=tree_df['y'], cmap='rainbow')\n",
                "\n",
                "# Create a legend object with title \"Classes\" and specified location on the plot\n",
                "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
                "ax.add_artist(legend)\n",
                "\n",
                "# Add labeling to the plot\n",
                "ax.set_xlabel('x1', fontsize='14')\n",
                "ax.set_ylabel('x2', fontsize='14')\n",
                "ax.set_title('Synthetic data with two classes', fontsize='15')\n",
                "\n",
                "# Display the plot\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1. Establish the Root Node.\n",
                "First, we need to decide what predictor to use for the 1st split and what threshold value we are splitting on. The choosen predictor \u0026 threshold will define the **Root Node** of our tree.\n",
                "\n",
                "For candidate splits we try every unique value of each of the predictors.\n",
                "\n",
                "To evaluate the candidate splits we'll use the Gini Impurity Index."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Remember that the Gini impurity index is defined as $1-(\\sum_{i=1}^J p_i^2)$ for all $J$ classes, where $p_i$ is the probability of choosing an item from class $i$."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Because we will compare our results to `sklearn` at the end, let's use the same conventions as their `DecisionTreeClassifier`.\n",
                "\n",
                "* Splitting conditions are stated in terms of \"\u003c= threshold\"\n",
                "* Points for which the splitting condition is **true** are placed in the **left** 'child' or 'leaf'. Those for which the condition is false are placed in the right child.\n",
                "\n",
                "This differs from the convention used in the lecture slides."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function that calculates the total Gini impurity index for each value provided\n",
                "def get_total_gini(predictor_values, pred_name, df):\n",
                "    '''\n",
                "    Parameters: an array of _unique_ predictor values,\n",
                "                a name of the predictor (String),\n",
                "                a corresponding dataframe object.\n",
                "    Returns: an array of total Gini index for each provided predictor value.\n",
                "    '''\n",
                "    total_gini = []\n",
                "    \n",
                "    # try each value as a potential split location\n",
                "    for val in predictor_values:\n",
                "    \n",
                "        # Left Leaf\n",
                "        # counts of each class (1 or 0) to the left of candidate split\n",
                "        left_1 = ___\n",
                "        left_0 = ___\n",
                "        \n",
                "        # total number of points on the left\n",
                "        # (max with small number to avoid devision by zero later)\n",
                "        N_left = max(1e-5, ___)\n",
                "\n",
                "        # Gini impurity for the left leaf\n",
                "        gini_left = ___\n",
                "    \n",
                "        # Right Leaf\n",
                "        # counts of each class (1 or 0) to the right of candidate split\n",
                "        right_1 = ___\n",
                "        right_0 = ___\n",
                "\n",
                "        # total number of points on the right\n",
                "        # (max with small number to avoid division by zero later)\n",
                "        N_right = max(1e-5, ___)\n",
                "        # Gini impurity for the right leaf\n",
                "        gini_right = ___\n",
                "\n",
                "        # total number of points in both leaves\n",
                "        N_total = ___\n",
                "        # total weighted gini impurity index\n",
                "        total_gini.append(___)\n",
                "    \n",
                "    return total_gini"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `get_total_gini` function takes *unique* predictor values as its first argument. Define these below (and consider why we want to use unique values here)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_x1_unique_values) ###\n",
                "\n",
                "# Get unique values of x1\n",
                "x1_unique = ___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_x2_unique_values) ###\n",
                "\n",
                "# Get unique values of x2\n",
                "x2_unique = ___"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now use `get_total_gini` to find the gini scores for each predictor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_x1_total_gini) ###\n",
                "\n",
                "# get total Gini index for x1\n",
                "x1_total_gini = ___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_x2_total_gini) ###\n",
                "\n",
                "# get total Gini index for x2\n",
                "x2_total_gini = ___"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Inspect the visualization of your results below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot the resulting total Gini indexes for each predictor\n",
                "plt.plot(x1_unique, x1_total_gini, 'r+', label=\"$x_1$\")\n",
                "plt.plot(x2_unique, x2_total_gini, 'b+', label=\"$x_2$\")\n",
                "plt.xlabel(\"Predictor values\", fontsize=\"13\")\n",
                "plt.ylabel(\"Total Gini index\", fontsize=\"13\")\n",
                "plt.title(\"Total gini indexes for x1 and x2 predictors - first split\", fontsize=\"14\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def report_splits(x1_unique, x2_unique, x1_total_gini, x2_total_gini):\n",
                "     # predictor providing the split with the lowest gini (x1=0, x2=1)\n",
                "     best_pred = np.argmin([min(x1_total_gini), min(x2_total_gini)])\n",
                "     # index of lowest gini score for best predictor \n",
                "     best_gini_idx = np.argmin([x1_total_gini, x2_total_gini][best_pred])\n",
                "     # best predictor value corresponding to that lowest gini score\n",
                "     best_pred_value = [x1_unique, x2_unique][best_pred][best_gini_idx]\n",
                "     print(\"The lowest total gini score is achieved when splitting on \"+\\\n",
                "          f\"{['x1','x2'][best_pred]} at the value {best_pred_value}.\")\n",
                "          \n",
                "report_splits(x1_unique, x2_unique, x1_total_gini, x2_total_gini)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Getting the 1st split threshold**\n",
                "\n",
                "The output above describes the best split among all the candidates. Each candidate split corresponded to a unique predictor value. **But this predictor value will not be our threshold value**. Instead, we use the midpoint *between* this value and the next highest unique value for that predictor in the training data (in this exercise, all data is training data)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_first_threshold) ###\n",
                "\n",
                "# Find the threshold to split on\n",
                "def get_threshold(unique_values, gini_scores):\n",
                "    # index of lowest gini score\n",
                "    idx = ___\n",
                "    # threshold is the midpoint between\n",
                "    # the predictor value resulting in lowest gini split\n",
                "    # and the next highest unique predictor value\n",
                "    # (you can assume values are sorted now and during gini calculation)\n",
                "    threshold = ___\n",
                "    return threshold\n",
                "\n",
                "x2_threshold = ___\n",
                "print(f\"Our threshold will be {x2_threshold}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2. Let's make our first split. \n",
                "\n",
                "We'll use some helper functions to assist with plotting the decision regions we get from our first split. \n",
                "\n",
                "`get_split_labels` determines the classification within each region based on the majority class within that region in the training data.\n",
                "\n",
                "`predict_class` takes vectors of `x1` and `x2` values and returns a vector of predicted class labels. We predict on dummy values created with `np.meshgrid` and use the results with `plt.contourf` to plot colored decision regions.\n",
                "\n",
                "In keeping with the convention used by `sklearn`, all points in our root node that are less than or equal to our threshold will be placed in the 'left' childe node. And the rest will go on in the 'right' child node. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_split_labels(splits):\n",
                "    '''\n",
                "    Parameters:\n",
                "        splits: List of ('predictor name', threshold) tuples\n",
                "                Ex: [('x1', 42), ('x2', 109)]\n",
                "    Returns: List of dictionaries, one for each split. \n",
                "             Dictionaries contain class labels for each side of the split\n",
                "    '''\n",
                "    split_labels = []\n",
                "    region = tree_df\n",
                "    for pred, thresh in splits:\n",
                "        region_labels = {\n",
                "            'left': region.loc[region[pred] \u003c= thresh, 'y'].mode().values[0],\n",
                "            'right': region.loc[region[pred] \u003e thresh, 'y'].mode().values[0]\n",
                "        }\n",
                "        split_labels.append(region_labels)\n",
                "        region = region[region[pred] \u003c= thresh]\n",
                "    return split_labels\n",
                "\n",
                "# Example of how get_split_labels works\n",
                "splits = [('x2', x2_threshold)]\n",
                "split_labels = get_split_labels(splits)\n",
                "print('class labels for the children of the root node:', split_labels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_class(x1, x2, splits):\n",
                "    # get split labels to use when predicting\n",
                "    split_labels = get_split_labels(splits)\n",
                "    y_hats = []\n",
                "    # iterate over each data point\n",
                "    for x1_i, x2_i in zip(x1.ravel(), x2.ravel()):\n",
                "        # dict lets us specify a predictor based on split rule\n",
                "        obs = {'x1': x1_i, 'x2': x2_i}\n",
                "        # apply each split rule\n",
                "        for n_split, (pred, thresh) in enumerate(splits):\n",
                "            # left\n",
                "            if obs[pred] \u003c= thresh:\n",
                "                if n_split == len(splits)-1:\n",
                "                    y_hats.append(split_labels[n_split]['left'])\n",
                "            # right\n",
                "            else:\n",
                "                y_hats.append(split_labels[n_split]['right'])\n",
                "                break\n",
                "    return np.array(y_hats)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we are ready to plot our split."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the split:\n",
                "# Create figure of specific size and the axes objects\n",
                "fig, ax = plt.subplots(figsize=(6, 6))\n",
                "\n",
                "# Scatter the data points\n",
                "# Other colormaps could be found here:\n",
                "# https://stackoverflow.com/questions/34314356/how-to-view-all-colormaps-available-in-matplotlib\n",
                "scatter = ax.scatter(tree_df['x1'], tree_df['x2'], c=tree_df['y'], cmap='rainbow')\n",
                "\n",
                "# Create a legend object with title \"Classes\" and specified location on the plot\n",
                "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
                "ax.add_artist(legend)\n",
                "\n",
                "# Split line\n",
                "ax.hlines(x2_threshold, xmin=0, xmax=500,\n",
                "          color ='black', lw = 2, ls=':', label='new split')\n",
                "ax.legend()\n",
                "\n",
                "# Add labeling to the plot\n",
                "ax.set_xlabel('x1', fontsize='14')\n",
                "ax.set_ylabel('x2', fontsize='14')\n",
                "ax.set_title('The first split of the data', fontsize='16')\n",
                "\n",
                "# Plot decision regions\n",
                "# dummy x1 \u0026 x2 grid values to predict on\n",
                "eps = 5 # padding for the grid\n",
                "xx1, xx2 = np.meshgrid(np.arange(tree_df['x1'].min()-eps, tree_df['x1'].max()+eps, 1),\n",
                "                       np.arange(tree_df['x2'].min()-eps, tree_df['x2'].max()+eps, 1))\n",
                "# predict class labels on grid points\n",
                "class_pred = predict_class(xx1, xx2, splits)\n",
                "# contour plot for decision regions\n",
                "plt.contourf(xx1, xx2, class_pred.reshape(xx1.shape), alpha=0.2, zorder=-1, cmap=plt.cm.coolwarm)\n",
                "\n",
                "# Display the plot\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll create a new DataFrame, `first_split_df` in which all `x2` values are less than or equal to `x2_threshold`.\n",
                "\n",
                "This DataFrame contains the points that end up in the region corresponding to the \"left leaf\" or \"left child\" of the root node. We'll use it in the next section where we further split this region.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "first_split_df  = ___\n",
                "# Peak at the result\n",
                "first_split_df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3. Making the second split."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we'll run the same function as above using our new df."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# provide unique values from our new dataframe\n",
                "x1_unique_2split = np.unique(first_split_df['x1'].values)\n",
                "x2_unique_2split = np.unique(first_split_df['x2'].values)\n",
                "\n",
                "tot_gini_x1_2split = get_total_gini(x1_unique_2split, 'x1', first_split_df)\n",
                "tot_gini_x2_2split = get_total_gini(x2_unique_2split, 'x2', first_split_df)\n",
                "\n",
                "plt.plot(x1_unique_2split, tot_gini_x1_2split, 'r+', label='$x_1$')\n",
                "plt.plot(x2_unique_2split, tot_gini_x2_2split, 'b+', label='$x_2$')\n",
                "plt.xlabel(\"Predictor values\", fontsize=\"13\")\n",
                "plt.ylabel(\"Total Gini index\", fontsize=\"13\")\n",
                "plt.title(\"Total Gini indexes for x1 and x2 predictors - second split\", fontsize=\"14\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "report_splits(x1_unique_2split, x2_unique_2split,\n",
                "              tot_gini_x1_2split, tot_gini_x2_2split)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_second_threshold) ###\n",
                "\n",
                "# The value to split on\n",
                "# Hint: be sure to use the variables defined \n",
                "# in the 2 cells above!\n",
                "x1_threshold_2split = ___\n",
                "x1_threshold_2split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the split:\n",
                "# Create figure of specific size and the axes objects\n",
                "fig, ax = plt.subplots(figsize=(6, 6))\n",
                "\n",
                "# Scatter the data points\n",
                "scatter = ax.scatter(tree_df['x1'], tree_df['x2'], c=tree_df['y'], cmap='rainbow')\n",
                "\n",
                "# Create a legend object with title \"Classes\" and specified location on the plot\n",
                "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
                "ax.add_artist(legend)\n",
                "\n",
                "# Split lines\n",
                "ax.hlines(x2_threshold, xmin=0, xmax=500,\n",
                "          color ='black', lw = 0.5, ls='--')\n",
                "ax.vlines(x1_threshold_2split, ymin=0, ymax=x2_threshold,\n",
                "          color ='black', lw = 2, ls=':', label='new split')\n",
                "ax.legend()\n",
                "\n",
                "# Plot decision regions\n",
                "# predict class labels on grid points\n",
                "class_pred = predict_class(xx1, xx2, [('x2', x2_threshold), ('x1', x1_threshold_2split)])\n",
                "# contour plot for decision regions\n",
                "plt.contourf(xx1, xx2, class_pred.reshape(xx1.shape), alpha=0.2, zorder=-1, cmap=plt.cm.coolwarm)\n",
                "\n",
                "# Add labeling to the plot\n",
                "ax.set_xlabel('x1', fontsize='13')\n",
                "ax.set_ylabel('x2', fontsize='13')\n",
                "ax.set_title('First and second splits on the data', fontsize='14')\n",
                "\n",
                "# Display the plot\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4. Stop splitting when the region formed is pure."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's split the data and check the values to the right of the threshold:\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "first_right_intern_node_df  = first_split_df[first_split_df['x1'] \u003e x1_threshold_2split]\n",
                "first_right_intern_node_df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see that this region is pure; all the class labels are `1`. There is no need to perform further splits on this region.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "first_right_intern_node_df['y'].value_counts()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5. Keep doing splits on impure regions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We stopped spliting on the right child of the second split because it was all one class, however the region represented by the left child is not pure.\n",
                "\n",
                "In the decision tree algorithm we would keep splitting it recursively till every resulting region will become pure or we encounter another stopping condition, such as max depth."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use the correct predictor and threshold to subset `first_split_df` to include only those points whose values are in the *left child* of our 2nd split for x1. \n",
                "\n",
                "Store the resulting dataframe as `second_split_left_df`.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "second_split_left_df  = ___"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Observe that *this* node is not pure:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "second_split_left_df['y'].value_counts()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's perform a final split on this impure region."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find the predictor and threshold for the split\n",
                "\n",
                "# get unique values of x1 and x2 in this region\n",
                "x1_unique_3split = np.unique(second_split_left_df['x1'].values)\n",
                "x2_unique_3split = np.unique(second_split_left_df['x2'].values)\n",
                "\n",
                "# Get total gini index for each predictor\n",
                "tot_gini_x1_3split = get_total_gini(x1_unique_3split, 'x1', second_split_left_df)\n",
                "tot_gini_x2_3split = get_total_gini(x2_unique_3split, 'x2', second_split_left_df)\n",
                "\n",
                "plt.plot(x1_unique_3split, tot_gini_x1_3split, 'r+', label='$x_1$')\n",
                "plt.plot(x2_unique_3split, tot_gini_x2_3split, 'b+', label='$x_2$')\n",
                "plt.xlabel(\"Predictor values\", fontsize=\"13\")\n",
                "plt.ylabel(\"Total Gini index\", fontsize=\"13\")\n",
                "plt.title(\"Total Gini indexes for x1 and x2 predictors - third split\", fontsize=\"14\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "report_splits(x1_unique_3split, x2_unique_3split,\n",
                "              tot_gini_x1_3split, tot_gini_x2_3split)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_third_threshold) ###\n",
                "\n",
                "# The value to split on\n",
                "x2_threshold_3split = ___\n",
                "x2_threshold_3split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the split:\n",
                "# Create figure of specific size and the axes objects\n",
                "fig, ax = plt.subplots(figsize=(6, 6))\n",
                "\n",
                "# Scatter the data points\n",
                "scatter = ax.scatter(tree_df['x1'], tree_df['x2'], c=tree_df['y'], cmap='rainbow')\n",
                "\n",
                "# Create a legend object with title \"Classes\" and specified location on the plot\n",
                "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n",
                "ax.add_artist(legend)\n",
                "\n",
                "# Split lines\n",
                "ax.hlines(x2_threshold, xmin=0, xmax=500,\n",
                "          color ='black', lw = 0.5, ls='--')\n",
                "ax.vlines(x1_threshold_2split, ymin=0, ymax=x2_threshold,\n",
                "          color ='black', lw = 0.5, ls='--')\n",
                "ax.hlines(x2_threshold_3split, xmin=0, xmax=x1_threshold_2split,\n",
                "          color ='black', lw = 2, ls=':', label='new split')\n",
                "ax.legend()\n",
                "\n",
                "# Plot decision regions\n",
                "# predict class labels on grid points\n",
                "class_pred = predict_class(xx1, xx2, [('x2', x2_threshold),\n",
                "                                      ('x1', x1_threshold_2split),\n",
                "                                      ('x2', x2_threshold_3split)])\n",
                "# contour plot for decision regions\n",
                "plt.contourf(xx1, xx2, class_pred.reshape(xx1.shape), alpha=0.2, zorder=-1, cmap=plt.cm.coolwarm)\n",
                "\n",
                "# Add labeling to the plot\n",
                "ax.set_xlabel('x1', fontsize='13')\n",
                "ax.set_ylabel('x2', fontsize='13')\n",
                "ax.set_title('First, second, and third splits on the data', fontsize='14')\n",
                "\n",
                "# Display the plot\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The two leaf nodes resulting from this split are pure!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the split\n",
                "# Get the left leaf node\n",
                "left_final_leaf_df  = second_split_left_df[second_split_left_df['x2'] \u003c= x2_threshold_3split]\n",
                "# Confirm the purity of the final leaf\n",
                "left_final_leaf_df['y'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the right leaf node\n",
                "right_final_leaf_df = second_split_left_df[second_split_left_df['x2'] \u003e x2_threshold_3split]\n",
                "# Confirm the putity of the final leaf\n",
                "right_final_leaf_df['y'].value_counts()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing to SKLearn\n",
                "We have just finished the left branch of the decision tree for this data. Now Let's compare our manual results with what sklearn DecisionTreeClassifier class would do. We will use a handy tree module from sklearn to visualize the produced tree."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data into predictor matrix and target series\n",
                "X = tree_df[['x1','x2']]\n",
                "y = tree_df['y']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit the tree to the data\n",
                "sklearn_tree = DecisionTreeClassifier(max_depth=3)\n",
                "sklearn_tree.fit(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the tree\n",
                "tree.plot_tree(sklearn_tree, fontsize=6)\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### As we can see above the left tree branch on the sklearn tree looks just like what we did manually above!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
